# HIPT(Hierarchical Image Pyramid Transformer)论文及库的理解应用

本库的原始代码请参照[HIPT](https://github.com/mahmoodlab/HIPT)，其对应的CVPR论文为[CVPR](https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Scaling_Vision_Transformers_to_Gigapixel_Images_via_Hierarchical_Self-Supervised_Learning_CVPR_2022_paper.pdf)。 编写此库的原因主要是为了记录学习使用的过程，这不是成熟的代码/笔记库，而是自己学习的过程。



## 摘要

Vision Transformers (ViTs)能够用于捕捉图片中的信息，但是对图像分辨率要求通常为256*256或384*384
在20倍镜下拍摄得到的病理图像的分辨率可以高达150000×150000, 展示出不同分辨率下视觉向量的分层结构，16×16的images展示的可能是单独的细胞，而4096×4096的image展示的可能是肿瘤微环境之间的关联
作者提出了HIPT结构，这个结构基于Vit，能够使用两个level的自监督学习去学习高分辨率图像的表征信息。
HIPT用涵盖33个肿瘤类型的10,678张WSIs，408,218张 4k图像和近1亿张256×256图像来预训练HIPT模型
HIPT在9个slide-level任务上进行了基准测试，得到的结果是HIPT在肿瘤分型和生存预测上表现最佳，且自监督ViTs能够对肿瘤微环境中表型的分层结构建模重要的归纳偏见。


## Introduction

### 1. 
多实例学习（MIL）的基本框架如下：1）在单一放大倍数视角下进行组织分割，2）patch级别的特征提取构建嵌入向量，以及3）向量的全局池化得到单一的slide级别的表征向量

这种三步框架有一定的设计缺陷。首先，分割和特征提取总是先定在256*256区域。虽然能勉强得到精细的形态学特征（例如核异型和肿瘤存在），但是这样的固定窗口在捕捉粗糙特征（例如肿瘤入侵、肿瘤大小、淋巴细胞浸润和更宽的空间组织结构）上语义有限。

相比其他基于image的序列建模方法（例如ViTs），因为WSI的序列长度太大，MIL只能使用全局池化操作符来聚合向量。这使得Transformer模型中的注意力机制不能应用于学习长距离的依赖关系，特别是设计肿瘤-免疫定位这样的表型时。长距离的依赖关系指的是在一个很大的空间或时间范围内相互作用或关联。例如在一张WSI中，一个区域的特征可能与另一个远离它的区域有关。

虽然近期的MIL方法采用了很多自监督学习作为patch级别特征提取的策略，聚合层中的参数仍然需要有监督的训练。考虑到与ViTs相关的WSIs的基于片段的序列建模时，我们注意到使用Transformer注意机制的架构设计选择使得在ViT模型中对标记化（tokenization）和聚合层（aggregation layers）进行预训练成为可能。这在防止多实例学习（MIL）模型在低数据量环境中过度拟合或欠拟合方面具有重要意义。

相比普通图像，我们注意到对WSI的建模得到的视觉向量通常是在一个固定放大倍数的情况下的一个固定尺度。例如，在20x倍镜下扫描的对象其尺度通常接近0.5μm每像素，这使得视觉向量可以进行一致的比较，揭示出超出其正常参考范围的重要组织形态学特征。此外，全切片图像在20×放大倍数下也展示了不同图像分辨率的视觉令牌的分层结构：16×16的图像包含了细胞和其他细粒度特征（基质、肿瘤细胞、淋巴细胞）的边界框；256×256的图像捕捉到细胞与细胞之间的局部集群互动（肿瘤细胞度）；1024×1024-4096×4096的图像进一步描述了细胞集群之间以及它们在组织中的宏观规模互动（描述肿瘤浸润性与肿瘤远离的淋巴细胞的肿瘤-免疫定位程度）；最后，全切片图像的幻灯片级别描绘了组织微环境内的整体肿瘤异质性。

因此介绍了一个基于Transformer的模型架构，能够将不同层级的视觉标记进行聚合并在完整病理图像上进行预训练。对slide级别的表征学习使用了类似于在语言模型中长文本表征的学习方式，具体来说，是一种三级的分层结构，从256×256和4096×4096窗口下最底层[16*16]视觉标记到最终形成slide级别的表征、

两个贡献：①把学习一张slide完整表征的任务分解成可用自监督学习学习的分层相关的表征；②我们使用DINO来对每个聚合层进行自监督的预训练，目标区域的尺寸可高达4096*4096

在需要高度以来上下文的任务中（例如生存预测），HIPT和传统MIL的差别特别明显。

通过在HIPT模型的4096×4096表征上使用KNN，在slide-level的分类任务上效果比很多弱监督模型还好

在组织病理学图像处理中，使用自监督的ViTs模型能够有效学习不同粒度的视觉概念，例如在ViT256-16模型上学习到细粒度视觉概念（细胞位置），而在ViT4096-256模型上学习到粗粒度视觉概念，例如更广泛的肿瘤细胞性。这一点在图3、图4中得以体现

### 2.
多实例学习实例有很多例子，最早的起源如何，最早在病理图像上的应用如何。
HIPT是在一个固定的放大倍数下来进行patch的，用较大的patch尺寸来捕捉更宏观尺度的形态学特征，有助于推动在WSI上进行上下文建模的思考
ViT很重要，目前该架构的最新进展主要集中在提高效率和整合多尺度信息。与病理学相比，如果图像尺度在给定的放大倍数下是固定的，那么学习尺度不变性可能没那么必要。NesT和Hierarchical Perceiver通过Transformer模块将非重叠的图像区域特征进行划分和聚合。HIPT的一个区别是在于每个阶段的ViT模块都可以进行单独预训练以实现高分辨率下的编码（4096×4096）


## Method

### 1. 问题构建
用下面的符号来区分“images”的尺寸和与“images”相对应的“tokens”。对一张大小为L×L的图像**x**(即**x**<sub>L</sub>)，我们会将该图像**x**<sub>L</sub>中非重叠的大小为l*l的patches中提取得到的视觉向量记为{**x**<sup>(i)</sup><sub>l</sub>}<sup>M</sup><sub>i=1</sub> ∈ R<sup>M×d<sub>l</sub></sup>，M是序列的长度而d是对l大小的标记中提取的嵌入维度。对**x**<sub>256</sub>大小的自然图像，ViTs通常会使用l=16来产生一个M=256的一个序列。此外，我们将在一个L大小的图像上的[l×l]标记工作的ViT命名为ViT<sub>L</sub>-l

对于一张具有y结果的WSI图像**x**<sub>WSI</sub>，目标通常是解决slide级别的分类问题 P(y|**x**<sub>WSI</sub>)。传统的方法是使用三步MIL框架：1）[256*256] patching; 2) 标记向量化和3）全局注意力池化。**x**<sub>WSI</sub>通常是以序列{**x**<sup>(i)</sup><sub>256</sub>}<sup>M</sup><sub>i=1</sub> ∈ R<sup>M×1024</sup>来表示，这来自于使用预训练的ResNet-50编码器来得到的。由于用l=256得到的序列向量很长，计算资源要求高，所以需要对神经网络结构加以限制以降低复杂性，方法包括逐块池化和全局池化，最后得到一个整张WSI的嵌入向量，用于下游任务。

### 2. HIPT构造
在改造ViTs以适应slide级别表征学习的过程中，再次需要重复两个最重要的挑战：1）视觉标记区域的固定尺度以及他们在不同图像分辨率下的层级关系和2）将WSI展开后得到的巨大向量长度。 病理图像中的视觉标记区域通常是以对象为中心的，在不同分辨率下有不同的粒度。这些视觉标记区域是有重要的上下文依赖性的，比如肿瘤-免疫和肿瘤-基质。在高分辨率下使用小视觉标记区域来patch会导致序列长度过大，使自注意力机制难以处理，而低分辨率下使用大视觉标记区域又会导致细节丢失，综合考量之后，仍然选择在20倍镜下使用[256*256]的patch操作。
为了捕捉这种分层结构及在各个图像分辨率上的重要依赖关系，我们将WSI视作类似于长文档的嵌套聚合形式的视觉标记区域，这些视觉区域不断递归细分为更小的视觉标记区域直到细胞级别：
HIPT(**x**<sub>WSI</sub>) = ViT<sub>WSI</sub>-4096({CLS<sup>(k)</sup><sub>4096</sub>}<sup>M</sup><sub>k=1</sub>) → CLS<sup>(k)</sup><sub>4096</sub> = ViT<sub>4096</sub>-256({CLS<sup>(j)</sup><sub>256</sub>}<sup>256</sup><sub>i=1</sub>) → CLS<sup>(j)</sup><sub>256</sub> = ViT<sub>256</sub>-16({**x**<sup>(i)</sup><sub>16</sub>}<sup>256</sup><sub>i=1</sub>)
