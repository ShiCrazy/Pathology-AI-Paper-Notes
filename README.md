# HIPT(Hierarchical Image Pyramid Transformer)论文及库的理解应用

本库的原始代码请参照[HIPT](https://github.com/mahmoodlab/HIPT)，其对应的CVPR论文为[CVPR](https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Scaling_Vision_Transformers_to_Gigapixel_Images_via_Hierarchical_Self-Supervised_Learning_CVPR_2022_paper.pdf)。 编写此库的原因主要是为了记录学习使用的过程，这不是成熟的代码/笔记库，而是自己学习的过程。



## 摘要

Vision Transformers (ViTs)能够用于捕捉图片中的信息，但是对图像分辨率要求通常为256*256或384*384
在20倍镜下拍摄得到的病理图像的分辨率可以高达150000×150000, 展示出不同分辨率下视觉向量的分层结构，16×16的images展示的可能是单独的细胞，而4096×4096的image展示的可能是肿瘤微环境之间的关联
作者提出了HIPT结构，这个结构基于Vit，能够使用两个level的自监督学习去学习高分辨率图像的表征信息。
HIPT用涵盖33个肿瘤类型的10,678张WSIs，408,218张 4k图像和近1亿张256×256图像来预训练HIPT模型
HIPT在9个slide-level任务上进行了基准测试，得到的结果是HIPT在肿瘤分型和生存预测上表现最佳，且自监督ViTs能够对肿瘤微环境中表型的分层结构建模重要的归纳偏见。


## Introduction

多实例学习（MIL）的基本框架如下：1）在单一放大倍数视角下进行组织分割，2）patch级别的特征提取构建嵌入向量，以及3）向量的全局池化得到单一的slide级别的表征向量

这种三步框架有一定的设计缺陷。首先，分割和特征提取总是先定在256*256区域。虽然能勉强得到精细的形态学特征（例如核异型和肿瘤存在），但是这样的固定窗口在捕捉粗糙特征（例如肿瘤入侵、肿瘤大小、淋巴细胞浸润和更宽的空间组织结构）上语义有限。

相比其他基于image的序列建模方法（例如ViTs），因为WSI的序列长度太大，MIL只能使用全局池化操作符来聚合向量。这使得Transformer模型中的注意力机制不能应用于学习长距离的依赖关系，特别是设计肿瘤-免疫定位这样的表型时。长距离的依赖关系指的是在一个很大的空间或时间范围内相互作用或关联。例如在一张WSI中，一个区域的特征可能与另一个远离它的区域有关。

虽然近期的MIL方法采用了很多自监督学习作为patch级别特征提取的策略，聚合层中的参数仍然需要有监督的训练。考虑到与ViTs相关的WSIs的基于片段的序列建模时，我们注意到使用Transformer注意机制的架构设计选择使得在ViT模型中对标记化（tokenization）和聚合层（aggregation layers）进行预训练成为可能。这在防止多实例学习（MIL）模型在低数据量环境中过度拟合或欠拟合方面具有重要意义。

相比普通图像，我们注意到对WSI的建模得到的视觉向量通常是在一个固定放大倍数的情况下的一个固定尺度。例如，在20x倍镜下扫描的对象其尺度通常接近0.5μm每像素，这使得视觉向量可以进行一致的比较，揭示出超出其正常参考范围的重要组织形态学特征。此外，全切片图像在20×放大倍数下也展示了不同图像分辨率的视觉令牌的分层结构：16×16的图像包含了细胞和其他细粒度特征（基质、肿瘤细胞、淋巴细胞）的边界框；256×256的图像捕捉到细胞与细胞之间的局部集群互动（肿瘤细胞度）；1024×1024-4096×4096的图像进一步描述了细胞集群之间以及它们在组织中的宏观规模互动（描述肿瘤浸润性与肿瘤远离的淋巴细胞的肿瘤-免疫定位程度）；最后，全切片图像的幻灯片级别描绘了组织微环境内的整体肿瘤异质性。

我们介绍了一个基于Transformer的模型架构，能够将不同层级的视觉标记进行聚合并在完整病理图像上进行预训练。
## 


